import { HttpClient, HttpHeaders } from '@angular/common/http';
import { Injectable } from '@angular/core';
import { Observable, throwError } from 'rxjs';
import { catchError, tap } from 'rxjs/operators';
import { environment } from '../../environments/environment';

interface LlmCompletionResponse {
  choices: Array<{
    message: {
      content: string;
      role: string;
    };
  }>;
}

@Injectable({
  providedIn: 'root'
})
export class LlmService {
  private apiUrl = 'https://api.openai.com/v1/chat/completions';
  private apiKey = environment.openaiApiKey;
  private debugMode = true; // Enable debug logging

  constructor(private http: HttpClient) {}
  getCompletion(prompt: string): Observable<LlmCompletionResponse> {
    // Check if API key is configured
    if (!this.apiKey || this.apiKey.trim() === '') {
      const errorMessage = 'OpenAI API key is not configured. Please add your API key to the environment configuration.';
      console.error('âŒ LLM Service:', errorMessage);
      return throwError(() => new Error(errorMessage));
    }

    // Note: Direct calls to OpenAI API from browser may face CORS issues
    // In production, consider using a backend proxy or serverless function
    if (this.debugMode) {
      console.log('ğŸš€ LLM Service: Sending request');
      console.log('ğŸ“ Prompt:', prompt);
      console.log('ğŸ”— API URL:', this.apiUrl);
      console.log('ğŸ”‘ API Key configured:', this.apiKey ? 'Yes' : 'No');
      console.log('ğŸ”‘ API Key length:', this.apiKey ? this.apiKey.length : 0);
      console.log('âš ï¸ Note: Browser-based API calls may face CORS restrictions');
    }

    const headers = new HttpHeaders({
      'Authorization': `Bearer ${this.apiKey}`,
      'Content-Type': 'application/json'
    });

    const body = {
      model: 'gpt-3.5-turbo', // Using more cost-effective model
      messages: [{ role: 'user', content: prompt }],
      temperature: 0.7
    };

    if (this.debugMode) {
      console.log('ğŸ“¦ Request body:', body);
      console.log('ğŸ”‘ Headers:', headers.keys());
    }

    return this.http.post<LlmCompletionResponse>(this.apiUrl, body, { headers }).pipe(
      tap((response: LlmCompletionResponse) => {
        if (this.debugMode) {
          console.log('âœ… LLM Service: Response received');
          console.log('ğŸ“¨ Response:', response);
          console.log('ğŸ’¬ Assistant reply:', response.choices?.[0]?.message?.content);
        }
      }),
      catchError(error => {
        console.error('âŒ LLM Service: Error occurred');
        console.error('ğŸ” Error details:', error);
        console.error('ğŸ“Š Error status:', error.status);
        console.error('ğŸ“„ Error message:', error.message);
        
        if (error.error) {
          console.error('ğŸš¨ API Error response:', error.error);
        }
        
        return throwError(() => error);
      })
    );
  }

  toggleDebugMode(): void {
    this.debugMode = !this.debugMode;
    console.log(`ğŸ”§ Debug mode ${this.debugMode ? 'enabled' : 'disabled'}`);
  }

  isDebugMode(): boolean {
    return this.debugMode;
  }
}
